"""Teaching assistant conversation orchestration."""

from __future__ import annotations

from collections.abc import Sequence
from dataclasses import asdict
from datetime import UTC, datetime
import json
from typing import Any

from pydantic import BaseModel, Field, ValidationError

from modules.conversation_engine.public import (
    BaseConversation,
    ConversationContext,
    ConversationMessageDTO,
    conversation_session,
)

from ..dtos import (
    TeachingAssistantContext,
    TeachingAssistantMessage,
    TeachingAssistantSessionState,
)


class TeachingAssistantResponse(BaseModel):
    """Structured response payload generated by the LLM."""

    message: str = Field(..., description="Assistant response to present to the learner.")
    suggested_quick_replies: list[str] = Field(
        default_factory=list,
        description="2-4 contextual follow-up suggestions for the learner.",
        max_length=4,
    )


class TeachingAssistantConversation(BaseConversation):
    """High-level dialog handler for the teaching assistant experience."""

    conversation_type = "teaching_assistant"
    system_prompt_file = "../prompts/teaching_assistant_system_prompt.md"

    _DEFAULT_QUICK_REPLIES: Sequence[str] = (
        "Can I get a hint?",
        "Explain this concept",
        "Walk me through an example",
        "I'm ready to continue",
    )

    @conversation_session
    async def start_session(
        self,
        *,
        unit_id: str,
        lesson_id: str | None,
        session_id: str | None,
        context: TeachingAssistantContext,
        _user_id: int | None = None,
        _conversation_metadata: dict[str, Any] | None = None,
        _conversation_title: str | None = None,
        _conversation_id: str | None = None,
    ) -> TeachingAssistantSessionState:
        """Start or resume a teaching assistant conversation."""

        metadata = self._build_metadata(unit_id, lesson_id, session_id)
        await self.update_conversation_metadata(metadata, merge=True)
        await self._ensure_opening_turn(context)
        return await self._build_session_state(context)

    @conversation_session
    async def submit_question(
        self,
        *,
        _conversation_id: str,
        message: str,
        context: TeachingAssistantContext,
        _user_id: int | None = None,
    ) -> TeachingAssistantSessionState:
        """Record a learner question and respond with contextual guidance."""

        metadata = self._build_metadata(context.unit_id, context.lesson_id, context.session_id)
        await self.update_conversation_metadata(metadata, merge=True)
        await self.record_user_message(message)
        await self._generate_assistant_reply(context, event="follow_up")
        return await self._build_session_state(context)

    @conversation_session
    async def get_session_state(
        self,
        *,
        _conversation_id: str,
        context: TeachingAssistantContext,
        _user_id: int | None = None,
    ) -> TeachingAssistantSessionState:
        """Return the latest session state without modifying conversation history."""

        metadata = self._build_metadata(context.unit_id, context.lesson_id, context.session_id)
        await self.update_conversation_metadata(metadata, merge=True)
        return await self._build_session_state(context)

    async def _ensure_opening_turn(self, context: TeachingAssistantContext) -> None:
        """Generate a contextual greeting if the conversation is empty."""

        ctx = ConversationContext.current()
        history = await ctx.service.get_message_history(ctx.conversation_id, include_system=False)
        has_assistant_message = any(message.role == "assistant" for message in history)
        if not has_assistant_message:
            # Add a placeholder user message to ensure LLM receives at least one user message
            # (required by LLM providers like Bedrock)
            await self.record_user_message("[Session opened]")
            await self._generate_assistant_reply(context, event="introduction")

    def _build_metadata(
        self,
        unit_id: str,
        lesson_id: str | None,
        session_id: str | None,
    ) -> dict[str, Any]:
        """Construct metadata snapshot stored alongside the conversation."""

        metadata: dict[str, Any] = {
            "unit_id": unit_id,
            "current_lesson_id": lesson_id,
            "current_session_id": session_id,
            "last_context_refreshed_at": datetime.now(UTC).isoformat(),
        }
        return metadata

    async def _generate_assistant_reply(
        self,
        context: TeachingAssistantContext,
        *,
        event: str,
    ) -> None:
        """Generate and persist an assistant reply using the LLM."""

        system_prompt = self._build_system_prompt(context, event=event)

        try:
            assistant_response, request_id, raw_response = await self.generate_structured_reply(
                TeachingAssistantResponse,
                model="claude-haiku-4-5",
                system_prompt=system_prompt,
            )
            quick_replies = self._normalize_quick_replies(assistant_response.suggested_quick_replies)
            provider = raw_response.get("provider", "openai")
            usage = raw_response.get("usage", {})
            cost_estimate = raw_response.get("cost_estimate")
        except ValidationError:
            quick_replies = self._build_default_quick_replies()
            provider = "fallback"
            request_id = None
            usage = {}
            cost_estimate = None
            assistant_response = TeachingAssistantResponse(
                message=self._fallback_message(event),
                suggested_quick_replies=list(quick_replies),
            )

        metadata: dict[str, Any] = {
            "provider": provider,
            "suggested_quick_replies": quick_replies,
        }

        await self.record_assistant_message(
            assistant_response.message,
            metadata=metadata,
            llm_request_id=request_id,
            tokens_used=usage.get("total_tokens"),
            cost_estimate=cost_estimate,
        )

    def _build_system_prompt(self, context: TeachingAssistantContext, *, event: str) -> str:
        """Combine the base system prompt with dynamic lesson context."""

        base_prompt = self.get_system_prompt()
        context_block = self._render_context_block(context)
        prompt = base_prompt.replace("{{CONTEXT}}", context_block)
        event_note = "The learner just opened the teaching assistant. Offer a contextual greeting that references their current progress." if event == "introduction" else "Respond to the latest learner message while maintaining continuity."
        return f"{prompt}\n\n# Event Guidance\n{event_note}"

    def _render_context_block(self, context: TeachingAssistantContext) -> str:
        """Serialize the teaching assistant context into a prompt-friendly JSON block."""

        raw = asdict(context)
        sanitized = {key: value for key, value in raw.items() if value not in (None, [], {})}
        return json.dumps(sanitized, ensure_ascii=False, indent=2, sort_keys=True)

    def _normalize_quick_replies(self, replies: Sequence[str]) -> list[str]:
        """Clean and constrain quick replies to the expected format."""

        cleaned: list[str] = []
        for reply in replies:
            text = str(reply).strip()
            if not text:
                continue
            if text in cleaned:
                continue
            cleaned.append(text[:60])
            if len(cleaned) == 4:
                break

        if len(cleaned) >= 2:
            return cleaned

        defaults = [candidate for candidate in self._build_default_quick_replies() if candidate not in cleaned]
        cleaned.extend(defaults)
        return cleaned[:4]

    def _build_default_quick_replies(self) -> list[str]:
        """Return a baseline set of quick replies when the LLM omits them."""

        return list(self._DEFAULT_QUICK_REPLIES[:4])

    def _fallback_message(self, event: str) -> str:
        """Fallback assistant message used when structured generation fails."""

        if event == "introduction":
            return "Hi! I'm here to help with this lesson. What part would you like to explore?"
        return "I'm ready to help. Tell me what feels tricky and we'll work through it together."

    async def _build_session_state(self, context: TeachingAssistantContext) -> TeachingAssistantSessionState:
        """Assemble the latest session state for API consumers."""

        ctx = ConversationContext.current()
        history = await ctx.service.get_message_history(ctx.conversation_id, include_system=False)
        summary = await self.get_conversation_summary()
        metadata = dict(summary.metadata or {})

        messages = [self._to_message(message) for message in history]
        suggested_quick_replies = self._extract_latest_quick_replies(messages)

        unit_id = metadata.get("unit_id") or context.unit_id
        lesson_id = metadata.get("current_lesson_id") or context.lesson_id
        session_id = metadata.get("current_session_id") or context.session_id

        enriched_context = TeachingAssistantContext(
            unit_id=unit_id,
            lesson_id=lesson_id,
            session_id=session_id,
            session=context.session,
            exercise_attempt_history=context.exercise_attempt_history,
            lesson=context.lesson,
            unit=context.unit,
            unit_session=context.unit_session,
            unit_resources=context.unit_resources,
        )

        return TeachingAssistantSessionState(
            conversation_id=str(ctx.conversation_id),
            unit_id=unit_id,
            lesson_id=lesson_id,
            session_id=session_id,
            messages=messages,
            suggested_quick_replies=suggested_quick_replies,
            metadata=metadata,
            context=enriched_context,
        )

    def _extract_latest_quick_replies(self, messages: Sequence[TeachingAssistantMessage]) -> list[str]:
        """Return quick replies from the most recent assistant message."""

        for message in reversed(messages):
            if message.role == "assistant" and message.suggested_quick_replies:
                return message.suggested_quick_replies
        return self._build_default_quick_replies()

    def _to_message(self, message: ConversationMessageDTO) -> TeachingAssistantMessage:
        """Convert a conversation engine DTO into a teaching assistant message."""

        metadata = dict(message.metadata or {})
        quick_replies_raw = metadata.get("suggested_quick_replies")
        quick_replies = self._normalize_quick_replies(quick_replies_raw or []) if quick_replies_raw else []

        return TeachingAssistantMessage(
            id=message.id,
            role=message.role,
            content=message.content,
            created_at=message.created_at,
            metadata=metadata,
            suggested_quick_replies=quick_replies,
        )


__all__ = ["TeachingAssistantConversation"]
