"""SQLAlchemy models for LLM services."""

from datetime import datetime
from typing import Any
import uuid

from sqlalchemy import (
    JSON,
    Boolean,
    DateTime,
    Float,
    Integer,
    String,
    Text,
    func,
)
from sqlalchemy.orm import Mapped, mapped_column

from modules.shared_models import Base, PostgresUUID

__all__ = ["LLMRequestModel"]


class LLMRequestModel(Base):
    """
    Tracks individual LLM API requests with full request/response data.

    This model provides comprehensive logging for LLM interactions including:
    - Complete request/response capture for debugging
    - Performance and cost analysis
    - Error tracking and retry information
    - Caching and optimization data
    """

    __tablename__ = "llm_requests"

    # Core identification
    # Unique identifier for this LLM request record
    id: Mapped[uuid.UUID] = mapped_column(PostgresUUID(), primary_key=True, default=uuid.uuid4)
    # Optional application user associated with this request (for tenancy/auditing)
    user_id: Mapped[int | None] = mapped_column(Integer, nullable=True, index=True)

    # API variant/versioning (e.g., Responses API)
    # Which high-level API was used to make this call (currently "responses")
    api_variant: Mapped[str] = mapped_column(String(50), nullable=False, default="responses", index=True)

    # Provider and model information
    # Provider key (e.g., "openai", "azure_openai") used to route the call
    provider: Mapped[str] = mapped_column(String(50), nullable=False, index=True)
    # Model name requested (e.g., "gpt-5", "gpt-5-mini")
    model: Mapped[str] = mapped_column(String(100), nullable=False, index=True)
    # Provider's response ID to cross-reference provider-side logs
    provider_response_id: Mapped[str | None] = mapped_column(String(100), nullable=True, index=True)
    # Provider system fingerprint/version marker for reproducibility
    system_fingerprint: Mapped[str | None] = mapped_column(String(100), nullable=True)

    # Request parameters
    # Sampling temperature used for this request
    temperature: Mapped[float] = mapped_column(Float, nullable=False)
    # Maximum number of tokens the model may generate (if specified)
    max_output_tokens: Mapped[int | None] = mapped_column(Integer, nullable=True)

    # Request and response data
    # Canonical, provider-agnostic conversation messages used to build the request
    messages: Mapped[list[dict[str, Any]]] = mapped_column(JSON, nullable=False)
    # Extra logical parameters not represented in top-level fields (kept for convenience)
    additional_params: Mapped[dict[str, Any] | None] = mapped_column(JSON, nullable=True)
    # Full, provider-shaped request as sent over the wire (authoritative for audit/repro)
    request_payload: Mapped[dict[str, Any] | None] = mapped_column(JSON, nullable=True)
    # Flattened textual content returned by the model (primary text output)
    response_content: Mapped[str | None] = mapped_column(Text, nullable=True)
    # Complete internal response DTO captured for reference/debugging
    response_raw: Mapped[dict[str, Any] | None] = mapped_column(JSON, nullable=True)
    # Provider-native structured output from the Responses API (e.g., output array)
    response_output: Mapped[dict[str, Any] | list[dict[str, Any]] | None] = mapped_column(JSON, nullable=True)

    # Usage and cost information
    # Total tokens reported by the provider (if provided as a single value)
    tokens_used: Mapped[int | None] = mapped_column(Integer, nullable=True)
    # Tokens consumed by the inputs portion of the request
    input_tokens: Mapped[int | None] = mapped_column(Integer, nullable=True)
    # Tokens generated by the model as output/completions
    output_tokens: Mapped[int | None] = mapped_column(Integer, nullable=True)
    # Estimated USD cost computed from token usage and model pricing
    cost_estimate: Mapped[float | None] = mapped_column(Float, nullable=True)

    # Response metadata
    # Provider-generated creation timestamp for the response (converted to aware datetime)
    response_created_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)

    # Execution status and timing
    # Current lifecycle status: "pending", "completed", or "failed"
    status: Mapped[str] = mapped_column(String(50), nullable=False, default="pending", index=True)  # pending, completed, failed
    # End-to-end time for the request in milliseconds
    execution_time_ms: Mapped[int | None] = mapped_column(Integer, nullable=True)

    # Error information
    # Error message captured when the request fails
    error_message: Mapped[str | None] = mapped_column(Text, nullable=True)
    # Python/Provider error type/class name for classification
    error_type: Mapped[str | None] = mapped_column(String(100), nullable=True)

    # Retry and optimization information
    # Which attempt this record represents (1 = initial call)
    retry_attempt: Mapped[int] = mapped_column(Integer, nullable=False, default=1)
    # Whether this response was returned from cache instead of a live API call
    cached: Mapped[bool] = mapped_column(Boolean, nullable=False, default=False, index=True)

    # Timing
    # Record creation time (database server clock, timezone-aware)
    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, server_default=func.now())
    updated_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False, server_default=func.now(), onupdate=func.now())

    def __repr__(self) -> str:
        return f"<LLMRequestModel(id={self.id}, provider='{self.provider}', model='{self.model}', status='{self.status}')>"

    @property
    def total_tokens_calculated(self) -> int | None:
        """Calculate total tokens if not provided."""
        if self.tokens_used is not None:
            return self.tokens_used
        # Prefer Responses API naming if available
        if self.input_tokens is not None and self.output_tokens is not None:
            return self.input_tokens + self.output_tokens
        return None

    @property
    def request_summary(self) -> dict[str, Any]:
        """Get a summary of the request for logging/debugging."""
        return {
            "id": str(self.id),
            "api_variant": self.api_variant,
            "provider": self.provider,
            "model": self.model,
            "provider_response_id": self.provider_response_id,
            "status": self.status,
            "tokens_used": self.tokens_used,
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "cost_estimate": self.cost_estimate,
            "execution_time_ms": self.execution_time_ms,
            "cached": self.cached,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "response_created_at": self.response_created_at.isoformat() if self.response_created_at else None,
        }
